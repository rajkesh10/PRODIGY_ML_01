# -*- coding: utf-8 -*-
"""House Prices - Regression Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/house-prices-regression-analysis-78aeeada-c29d-4506-aa20-bb9463dea1fb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240401/auto/storage/goog4_request%26X-Goog-Date%3D20240401T080859Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3e7c73a27bf9ef719c3a5a2f54c49f239f74b8c6cc8adfa1e29a410758cd2c64bd853724a40fb224990610df8fa6997a7660a3b82f8a08131b35c4ce64b6899ce3783aabd46b21bdbbc60f32c77cb43a5f3e03b3f460075a22f2fe8305bc0a6b923700a705f10acb68526b6affb88fe1b1995b5754c4c7aed78c6b5d7080b4025a9729ecbaadf752307e05e0ee6fe8d0eec223eeef3d505fd7c2cc17db231259f352cb31cc994a8357fc0a554193f1b86a5aee7a47651480ff9ec036a3b3cd786b6081b54c43feb0c7ae1f6f77eff6adbbe3a0ffb67af22076358d267b6e1728a8b8d33883e6b77a118888d5788a567cda29c0fbaca2e1d7ced517d47702ee69
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data
df_train = pd.read_csv("/content/train.csv")
df_test = pd.read_csv("/content/test.csv")

!pip install scikit-learn

from sklearn.preprocessing import LabelEncoder
import pandas as pd

unseen_categories = set(df_test[categorical_cols].values.ravel()) - set(df_train[categorical_cols].values.ravel())
print(f"Unseen categories: {unseen_categories}")

print(unseen_categories)

print(df_test.columns)

print(df_test.isnull().sum())

# Feature engineering (replace with more advanced techniques as needed)
# Handle categorical features
categorical_cols = [col for col in df_train.columns if df_train[col].dtype == 'object']
le = LabelEncoder()
le.fit(df_train[categorical_cols].values.ravel())  # Fit on all categories (including unseen)

def encode_label(col):
  # Encode with le, unseen labels get -1
  return le.transform(col)

df_train[categorical_cols] = df_train[categorical_cols].apply(encode_label)
df_test[categorical_cols] = df_test[categorical_cols].apply(encode_label)

# Handle missing values (replace with more sophisticated methods)
df_train.fillna(df_train.mean(), inplace=True)
df_test.fillna(df_train.mean(), inplace=True)

# Separate features and target variable
X_train = df_train.drop('SalePrice', axis=1)
y_train = df_train['SalePrice']

# Train-Test Split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Random Forest Regression
model_rf = RandomForestRegressor(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)  # Colon added after function definition
y_pred_rf = model_rf.predict(X_val)  # Colon added after function definition
rf_mse = mean_squared_error(y_val, y_pred_rf)
print(f"Random Forest MSE: {rf_mse}")

# Separate features and target variable (drop "Id" consistently)
X_train = df_train.drop(['Id', 'SalePrice'], axis=1)
y_train = df_train['SalePrice']

# Train-Test Split (already done, but ensuring consistency)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
model_rf.fit(X_train, y_train)

# Scatter plot of actual vs predicted values
plt.scatter(y_val, y_pred_rf)
plt.xlabel("Actual Sale Price")
plt.ylabel("Predicted Sale Price")
plt.title("Random Forest Regression - Actual vs Predicted")
plt.show()

# Prediction on Test Data (assuming model_rf is already trained)
X_test = df_test.drop('Id', axis=1)  # Drop ID as it's not a feature
y_pred_test_rf = model_rf.predict(X_test)

# Create the submission DataFrame
submission_df = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_pred_test_rf})

# Save predictions for submission
submission_df.to_csv('submission.csv', index=False)